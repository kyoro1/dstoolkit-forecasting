{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc3e4402",
      "metadata": {},
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sWbXCGozBRNW",
      "metadata": {
        "id": "sWbXCGozBRNW"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "kmxpysFu7zjH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmxpysFu7zjH",
        "outputId": "db2717d5-22be-4fa8-99fb-3f9ea90e7e1b"
      },
      "outputs": [],
      "source": [
        "# data elaboration functions\n",
        "import pandas as pd\n",
        "from six.moves import collections_abc\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "# datetime functions\n",
        "import datetime as dt\n",
        "\n",
        "# file management functions\n",
        "import os\n",
        "import sys\n",
        "import opendatasets as od\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# plot functions\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# data science functions\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import joblib\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# configuration file\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "from Configuration.config import cfg_path\n",
        "\n",
        "# custom functions\n",
        "from Code.Plotting.plots import Plots\n",
        "from Code.Regressors.regressors import Regressors\n",
        "from Code.Scoring.scoring import Scoring\n",
        "from Code.Scoring.train_test import TrainTest\n",
        "from Code.Scoring.train import Training\n",
        "from Code.Scoring.forecast import Forecasting\n",
        "from Code.Scoring.kpi import Kpi\n",
        "from Code.Scoring.scoring import Scoring\n",
        "from Code.Utils.utils import Utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc26b7b",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "458162d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#od.download(\"https://www.kaggle.com/arashnic/building-sites-power-consumption-dataset/download\")\n",
        "root = Path(os.getcwd()).parent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86bb0e13",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "09358d6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "find_date, date_col found: ['timestamp']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>obs_id</th>\n",
              "      <th>forecast_id</th>\n",
              "      <th>value</th>\n",
              "      <th>holidays</th>\n",
              "      <th>day_off</th>\n",
              "      <th>surface</th>\n",
              "      <th>base_temperature</th>\n",
              "      <th>wd_mon</th>\n",
              "      <th>...</th>\n",
              "      <th>month_05</th>\n",
              "      <th>month_06</th>\n",
              "      <th>month_07</th>\n",
              "      <th>month_08</th>\n",
              "      <th>month_09</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "      <th>month_12</th>\n",
              "      <th>months_days</th>\n",
              "      <th>temperature_asis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>7390465.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>1.492533e+06</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6098.278376</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11/02</td>\n",
              "      <td>13.247917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10556.293605</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11/02</td>\n",
              "      <td>13.252083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12541.181277</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11/02</td>\n",
              "      <td>13.241667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>1413383.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>8.541479e+05</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9150.195373</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11/02</td>\n",
              "      <td>13.243750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15168.125971</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11/02</td>\n",
              "      <td>13.245833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   site_id  timestamp     obs_id  forecast_id         value  holidays  \\\n",
              "0        2 2015-11-02  7390465.0         26.0  1.492533e+06         0   \n",
              "1        3 2015-11-02        NaN          NaN           NaN         0   \n",
              "2        5 2015-11-02        NaN          NaN           NaN         0   \n",
              "3        6 2015-11-02  1413383.0        129.0  8.541479e+05         0   \n",
              "4        7 2015-11-02        NaN          NaN           NaN         0   \n",
              "\n",
              "   day_off       surface  base_temperature  wd_mon  ...  month_05  month_06  \\\n",
              "0        0   6098.278376              18.0       1  ...         0         0   \n",
              "1        0  10556.293605              18.0       1  ...         0         0   \n",
              "2        0  12541.181277              18.0       1  ...         0         0   \n",
              "3        0   9150.195373              18.0       1  ...         0         0   \n",
              "4        0  15168.125971              18.0       1  ...         0         0   \n",
              "\n",
              "   month_07  month_08  month_09  month_10  month_11  month_12  months_days  \\\n",
              "0         0         0         0         0         1         0        11/02   \n",
              "1         0         0         0         0         1         0        11/02   \n",
              "2         0         0         0         0         1         0        11/02   \n",
              "3         0         0         0         0         1         0        11/02   \n",
              "4         0         0         0         0         1         0        11/02   \n",
              "\n",
              "   temperature_asis  \n",
              "0         13.247917  \n",
              "1         13.252083  \n",
              "2         13.241667  \n",
              "3         13.243750  \n",
              "4         13.245833  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_final = pd.read_pickle(os.path.join(root, cfg_path.data_dir.output_path, 'df_final.pkl'))\n",
        "date_var = Utils.find_date(df_final)\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5884738d",
      "metadata": {},
      "source": [
        "# Define model_01_thermal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f23ed7fb",
      "metadata": {},
      "source": [
        "## Parameter setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a9a6af5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_models = {}\n",
        "m = 'model_01_thermal'\n",
        "dict_models[m] = {}\n",
        "dict_models[m]['id'] = 'site_id'\n",
        "dict_models[m]['list_unique_id'] = ['site_id', 'timestamp']\n",
        "dict_models[m]['y'] = 'value'\n",
        "\n",
        "# If the following are ='', it will take the latest year as test set and the previous year as train set\n",
        "dict_models[m]['train_start_date'] = ''\n",
        "dict_models[m]['train_end_date'] = ''\n",
        "dict_models[m]['test_start_date'] = ''\n",
        "dict_models[m]['test_end_date'] = ''\n",
        "dict_models[m]['test_size'] = 0.33"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "146e2f2d",
      "metadata": {},
      "source": [
        "## Regressors dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "10f1fc4b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['site_id', 'timestamp', 'obs_id', 'forecast_id', 'value', 'holidays',\n",
              "       'day_off', 'surface', 'base_temperature', 'wd_mon', 'wd_tue', 'wd_wed',\n",
              "       'wd_thu', 'wd_fri', 'wd_sat', 'wd_sun', 'month_01', 'month_02',\n",
              "       'month_03', 'month_04', 'month_05', 'month_06', 'month_07', 'month_08',\n",
              "       'month_09', 'month_10', 'month_11', 'month_12', 'months_days',\n",
              "       'temperature_asis'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Have a look at the available regressors\n",
        "df_final.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e65e2014",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regressor holidays has all needed values\n",
            "Regressor day_off has all needed values\n",
            "Regressor surface has all needed values\n",
            "Regressor base_temperature has all needed values\n",
            "Regressor wd_mon has all needed values\n",
            "Regressor wd_tue has all needed values\n",
            "Regressor wd_wed has all needed values\n",
            "Regressor wd_thu has all needed values\n",
            "Regressor wd_fri has all needed values\n",
            "Regressor wd_sat has all needed values\n",
            "Regressor wd_sun has all needed values\n",
            "Regressor month_01 has all needed values\n",
            "Regressor month_02 has all needed values\n",
            "Regressor month_03 has all needed values\n",
            "Regressor month_04 has all needed values\n",
            "Regressor month_05 has all needed values\n",
            "Regressor month_06 has all needed values\n",
            "Regressor month_07 has all needed values\n",
            "Regressor month_08 has all needed values\n",
            "Regressor month_09 has all needed values\n",
            "Regressor month_10 has all needed values\n",
            "Regressor month_11 has all needed values\n",
            "Regressor month_12 has all needed values\n",
            "Regressor months_days has all needed values\n",
            "Regressor temperature_asis has all needed values\n"
          ]
        }
      ],
      "source": [
        "# Check regressors availability\n",
        "regressors_list = [ 'holidays',\n",
        "       'day_off', 'surface', 'base_temperature', 'wd_mon', 'wd_tue', 'wd_wed',\n",
        "       'wd_thu', 'wd_fri', 'wd_sat', 'wd_sun', 'month_01', 'month_02',\n",
        "       'month_03', 'month_04', 'month_05', 'month_06', 'month_07', 'month_08',\n",
        "       'month_09', 'month_10', 'month_11', 'month_12', 'months_days',\n",
        "       'temperature_asis']\n",
        "\n",
        "forecast_end_date = df_final[date_var].max()\n",
        "Utils.check_regressors_availability(df_final, date_var, regressors_list, forecast_end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4be2b20d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile dictionary of regressors\n",
        "dict_regressors = {             \n",
        "    'list_temp': ['temperature_asis'],\n",
        "    'holidays': ['holidays'],\n",
        "    'wd': ['wd_fri', 'wd_mon', 'wd_tue', 'wd_sat', 'wd_sun', 'wd_thu'],\n",
        "    'month': ['month_01', 'month_02', 'month_03', 'month_04', 'month_05', 'month_07', 'month_08', 'month_09', 'month_10', 'month_11', 'month_12'],\n",
        "    'additional_regressors': []}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ad21db",
      "metadata": {},
      "source": [
        "#### Interaction terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "14ab926f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add optional interaction terms\n",
        "dict_interactions = {'set_01': {'reg_list_01': 'list_temp', 'reg_list_02': 'month'}}\n",
        "for r in list(dict_interactions.keys()):\n",
        "    list_element = list(dict_interactions[r].keys())\n",
        "    if len(list_element)==2:\n",
        "        reg_list_01 = dict_interactions[r][list_element[0]] \n",
        "        reg_list_02 = dict_interactions[r][list_element[1]]\n",
        "        for i in dict_regressors[reg_list_01]:\n",
        "            for j in dict_regressors[reg_list_02]:\n",
        "                Regressors.create_interactions(df_final, i, j)\n",
        "    else:\n",
        "        print('Define model: list of elements in interactions is more than 2', list_element)\n",
        "\n",
        "list_interactions = list(df_final.filter(like='*').columns)\n",
        "for e in list_interactions:\n",
        "    dict_regressors['additional_regressors'].append(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6945c3d0",
      "metadata": {},
      "source": [
        "#### Non linear terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "39e6b24d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add optional non linear terms\n",
        "dict_non_linear_terms = {'set_01':{'temperature_asis': 2}}\n",
        "for r in list(dict_non_linear_terms.keys()):\n",
        "    list_element = list(dict_non_linear_terms[r].keys())\n",
        "    for e in list_element:\n",
        "        if len(list_element)==1:\n",
        "            var = list(dict_non_linear_terms[r].keys())[0]\n",
        "            n = dict_non_linear_terms[r][e]\n",
        "            Regressors.create_non_linear_terms(df_final, var, n)\n",
        "        else:\n",
        "            print('Define model: list of elements in non linear terms is more than 1', r, list_element)\n",
        "        \n",
        "list_non_linear_terms = list(df_final.filter(like='^').columns)\n",
        "for e in list_non_linear_terms:\n",
        "    dict_regressors['additional_regressors'].append(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021524df",
      "metadata": {},
      "source": [
        "## Algorithms dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "37117f23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define algorithms to test, these algorithms will be used in backtesting\n",
        "n_jobs = -1\n",
        "dict_algorithms = {}\n",
        "dict_algorithms['RF_Regressor'] = RandomForestRegressor(n_estimators=200, max_depth = 10, random_state =0, n_jobs=n_jobs)\n",
        "dict_algorithms['LR_Regressor'] = LinearRegression(n_jobs=n_jobs)\n",
        "dict_algorithms['XGB_Regressor'] = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.5,\n",
        "                                max_depth = 5, alpha = 10, n_estimators = 50)\n",
        "\n",
        "# Always add a forecasting algorithm used to produce forecasts out of sample\n",
        "dict_algorithms['out_of_sample'] = dict_algorithms['LR_Regressor']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217e2ff3",
      "metadata": {},
      "source": [
        "## Addind regressors and algorithms to model dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "04ac8962",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_models is the following: {'model_01_thermal': {'id': 'site_id', 'list_unique_id': ['site_id', 'timestamp'], 'y': 'value', 'train_start_date': '', 'train_end_date': '', 'test_start_date': '', 'test_end_date': '', 'test_size': 0.33, 'regressors': {'list_temp': ['temperature_asis'], 'holidays': ['holidays'], 'wd': ['wd_fri', 'wd_mon', 'wd_tue', 'wd_sat', 'wd_sun', 'wd_thu'], 'month': ['month_01', 'month_02', 'month_03', 'month_04', 'month_05', 'month_07', 'month_08', 'month_09', 'month_10', 'month_11', 'month_12'], 'additional_regressors': ['temperature_asis*month_01', 'temperature_asis*month_02', 'temperature_asis*month_03', 'temperature_asis*month_04', 'temperature_asis*month_05', 'temperature_asis*month_07', 'temperature_asis*month_08', 'temperature_asis*month_09', 'temperature_asis*month_10', 'temperature_asis*month_11', 'temperature_asis*month_12', 'temperature_asis^2']}, 'algorithms': {'RF_Regressor': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
            "                      max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
            "                      max_samples=None, min_impurity_decrease=0.0,\n",
            "                      min_impurity_split=None, min_samples_leaf=1,\n",
            "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
            "                      n_estimators=200, n_jobs=-1, oob_score=False,\n",
            "                      random_state=0, verbose=0, warm_start=False), 'LR_Regressor': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False), 'XGB_Regressor': XGBRegressor(alpha=10, base_score=None, booster=None, colsample_bylevel=None,\n",
            "             colsample_bynode=None, colsample_bytree=0.3, gamma=None,\n",
            "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
            "             learning_rate=0.5, max_delta_step=None, max_depth=5,\n",
            "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "             n_estimators=50, n_jobs=None, num_parallel_tree=None,\n",
            "             objective='reg:squarederror', random_state=None, reg_alpha=None,\n",
            "             reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
            "             tree_method=None, validate_parameters=None, verbosity=None), 'out_of_sample': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)}}}\n"
          ]
        }
      ],
      "source": [
        "dict_models['model_01_thermal']['regressors'] = dict_regressors\n",
        "dict_models['model_01_thermal']['algorithms'] = dict_algorithms\n",
        "print('dict_models is the following:', dict_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SnZLgs7EAatB",
      "metadata": {
        "id": "SnZLgs7EAatB"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01cc7822",
      "metadata": {},
      "source": [
        "### Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7E5fWofhAZKe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7E5fWofhAZKe",
        "outputId": "184c426a-9a85-48ec-98f9-6cbb39684b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "find_date, date_col found: ['timestamp']\n"
          ]
        }
      ],
      "source": [
        "date_var = Utils.find_date(df_final)\n",
        "list_unique_id = dict_models['model_01_thermal']['list_unique_id']\n",
        "id = dict_models['model_01_thermal']['id']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8feed5d5",
      "metadata": {},
      "source": [
        "### Site stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b85e4455",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SitesIds with most samples [42, 19, 22, 50, 49]\n"
          ]
        }
      ],
      "source": [
        "site_stats = Scoring.stats_per_site(df_final, id, date_var)\n",
        "\n",
        "# Selecting Sites with most samples\n",
        "top5_sites = site_stats.iloc[:5][id]\n",
        "print(\"SitesIds with most samples\", top5_sites.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e984b",
      "metadata": {},
      "source": [
        "### Training dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bdR2WHnTiaUa",
      "metadata": {
        "id": "bdR2WHnTiaUa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Actual id list: [19, 22, 42, 49, 50]\n",
            "Actual regressors available: ['day_off', 'month_12', 'months_days', 'temperature_asis*month_05', 'month_10', 'wd_wed', 'temperature_asis^2', 'wd_sat', 'wd_tue', 'forecast_id', 'month_07', 'temperature_asis*month_09', 'temperature_asis*month_10', 'obs_id', 'temperature_asis*month_08', 'month_05', 'temperature_asis*month_01', 'temperature_asis*month_02', 'month_09', 'temperature_asis*month_07', 'month_04', 'month_02', 'temperature_asis*month_04', 'temperature_asis*month_12', 'temperature_asis', 'wd_fri', 'wd_mon', 'surface', 'value', 'month_08', 'temperature_asis*month_03', 'holidays', 'month_03', 'wd_thu', 'month_11', 'month_01', 'wd_sun', 'base_temperature', 'temperature_asis*month_11', 'month_06']\n"
          ]
        }
      ],
      "source": [
        "df = df_final.loc[(df_final[id].isin(top5_sites)), ]\n",
        "print('Actual id list:', list(df[id].unique()))\n",
        "print('Actual regressors available:', list(set(list(df.columns)) - set(list_unique_id)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258e375b",
      "metadata": {},
      "source": [
        "# Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8ffb286c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forecasting with model model_01_thermal\n",
            "Actual regressors used in model: model_01_thermal ['temperature_asis', 'holidays', 'wd_fri', 'wd_mon', 'wd_tue', 'wd_sat', 'wd_sun', 'wd_thu', 'month_01', 'month_02', 'month_03', 'month_04', 'month_05', 'month_07', 'month_08', 'month_09', 'month_10', 'month_11', 'month_12', 'temperature_asis*month_01', 'temperature_asis*month_02', 'temperature_asis*month_03', 'temperature_asis*month_04', 'temperature_asis*month_05', 'temperature_asis*month_07', 'temperature_asis*month_08', 'temperature_asis*month_09', 'temperature_asis*month_10', 'temperature_asis*month_11', 'temperature_asis*month_12', 'temperature_asis^2']\n",
            "Forecasting site_id 19\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Train start date is already a date\n",
            "Train start date is 2015-11-02 00:00:00\n",
            "Train end date is already a date\n",
            "Train end date is 2017-03-06 00:00:00\n",
            "Train shape before removing nan is 491\n",
            "Min date AFTER removing nan is 2015-11-02 00:00:00\n",
            "Max date AFTER removing nan is 2017-03-06 00:00:00\n",
            "Shape AFTER removing nan is 491\n",
            "find_date, date_col found: ['timestamp']\n",
            "Test start date is 2017-03-07 00:00:00\n",
            "Test end date is 2022-12-31 00:00:00\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model RF_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model LR_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model XGB_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model out_of_sample\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "site_id 19 forecasting completed with model model_01_thermal\n",
            "Forecasting site_id 22\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Train start date is already a date\n",
            "Train start date is 2015-11-02 00:00:00\n",
            "Train end date is already a date\n",
            "Train end date is 2017-03-06 00:00:00\n",
            "Train shape before removing nan is 491\n",
            "Min date AFTER removing nan is 2015-11-02 00:00:00\n",
            "Max date AFTER removing nan is 2017-03-06 00:00:00\n",
            "Shape AFTER removing nan is 491\n",
            "find_date, date_col found: ['timestamp']\n",
            "Test start date is 2017-03-07 00:00:00\n",
            "Test end date is 2022-12-31 00:00:00\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model RF_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model LR_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model XGB_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model out_of_sample\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "site_id 22 forecasting completed with model model_01_thermal\n",
            "Forecasting site_id 42\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Train start date is already a date\n",
            "Train start date is 2015-11-02 00:00:00\n",
            "Train end date is already a date\n",
            "Train end date is 2017-03-26 00:00:00\n",
            "Train shape before removing nan is 511\n",
            "Min date AFTER removing nan is 2015-11-02 00:00:00\n",
            "Max date AFTER removing nan is 2017-03-26 00:00:00\n",
            "Shape AFTER removing nan is 511\n",
            "find_date, date_col found: ['timestamp']\n",
            "Test start date is 2017-03-27 00:00:00\n",
            "Test end date is 2022-12-31 00:00:00\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model RF_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model LR_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model XGB_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model out_of_sample\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "site_id 42 forecasting completed with model model_01_thermal\n",
            "Forecasting site_id 49\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Train start date is already a date\n",
            "Train start date is 2015-11-19 00:00:00\n",
            "Train end date is already a date\n",
            "Train end date is 2017-03-15 00:00:00\n",
            "Train shape before removing nan is 483\n",
            "Min date AFTER removing nan is 2015-11-19 00:00:00\n",
            "Max date AFTER removing nan is 2017-03-15 00:00:00\n",
            "Shape AFTER removing nan is 483\n",
            "find_date, date_col found: ['timestamp']\n",
            "Test start date is 2017-03-16 00:00:00\n",
            "Test end date is 2022-12-31 00:00:00\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model RF_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model LR_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model XGB_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model out_of_sample\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "site_id 49 forecasting completed with model model_01_thermal\n",
            "Forecasting site_id 50\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Train start date is already a date\n",
            "Train start date is 2015-11-18 00:00:00\n",
            "Train end date is already a date\n",
            "Train end date is 2017-03-14 00:00:00\n",
            "Train shape before removing nan is 483\n",
            "Min date AFTER removing nan is 2015-11-18 00:00:00\n",
            "Max date AFTER removing nan is 2017-03-14 00:00:00\n",
            "Shape AFTER removing nan is 483\n",
            "find_date, date_col found: ['timestamp']\n",
            "Test start date is 2017-03-15 00:00:00\n",
            "Test end date is 2022-12-31 00:00:00\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model RF_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model LR_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model XGB_Regressor\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "kpi for model out_of_sample\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "find_date, date_col found: ['timestamp']\n",
            "Training\n",
            "Training completed\n",
            "find_date, date_col found: ['timestamp']\n",
            "Forecasting\n",
            "Forecasting completed\n",
            "site_id 50 forecasting completed with model model_01_thermal\n"
          ]
        }
      ],
      "source": [
        "dict_results = {}\n",
        "for m in list(dict_models.keys()):\n",
        "    print('Forecasting with model', m)\n",
        "    dict_results[m] = {}\n",
        "    \n",
        "    # Get dict of algorithms\n",
        "    dict_algorithms = dict_models[m]['algorithms']    \n",
        "    \n",
        "    # Get list of algorithms\n",
        "    list_regressors = []\n",
        "    for reg in list(dict_models[m]['regressors'].keys()):\n",
        "        list_regressors = list_regressors + dict_models[m]['regressors'][reg]\n",
        "        \n",
        "    print('Actual regressors used in model:', m, list_regressors)\n",
        "        \n",
        "    # Define columns to keep: list unique id, y and list of regressors\n",
        "    list_unique_id = dict_models[m]['list_unique_id'] \n",
        "    y = dict_models[m]['y']\n",
        "    id = dict_models[m]['id']\n",
        "    cols_to_keep = dict_models[m]['list_unique_id'] + [y] + list_regressors \n",
        "\n",
        "    for s in list(df[id].unique()):\n",
        "        print('Forecasting', id, s)\n",
        "        dict_results[m][s] = {}\n",
        "        df_sliced = df.loc[df[id]==s, cols_to_keep].copy()\n",
        "        \n",
        "        # Adding train and test set dates\n",
        "        dict_train_test_set = TrainTest.define_train_test_set_dates(df_sliced, y, dict_models[m]['train_start_date'], dict_models[m]['train_end_date'], dict_models[m]['test_start_date'], dict_models[m]['test_end_date'], dict_models[m]['test_size'])\n",
        "                \n",
        "        # Scoring\n",
        "        try:\n",
        "            dict_train = TrainTest.def_train(df_sliced, y, list_unique_id, dict_train_test_set['train_start_date'], dict_train_test_set['train_end_date'])\n",
        "            dict_test = TrainTest.def_test(df_sliced, y, list_unique_id, dict_train_test_set['test_start_date'], dict_train_test_set['test_end_date'])\n",
        "            best_algorithm = Scoring.find_best_algorithm(y, dict_train, dict_test, dict_algorithms, out_of_sample='out_of_sample')\n",
        "            trained_model = Training.train(dict_train, dict_algorithms[best_algorithm])\n",
        "            forecasted_model = Forecasting.forecast(dict_test, trained_model)\n",
        "            dict_results[m][s] = {'best_algorithm': best_algorithm, 'historical_data': dict_train['historical_data'], 'forecast': forecasted_model['df_fcst'], 'train_start_date': dict_train_test_set['train_start_date'], 'train_end_date': dict_train_test_set['train_end_date'], 'test_start_date': dict_train_test_set['test_start_date'], 'test_end_date': dict_train_test_set['test_end_date']}\n",
        "            print(id, s, 'forecasting completed with model', m)\n",
        "        except ValueError as e:\n",
        "            raise Exception(id, s, 'could not be forecasted', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dWZkD8v6RLwk",
      "metadata": {
        "id": "dWZkD8v6RLwk"
      },
      "source": [
        "# Finalize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cbacdeb",
      "metadata": {},
      "source": [
        "### Create csv as per input format of PowerBI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "060959d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing PBI for site_id 19\n",
            "Preparing PBI for site_id 22\n",
            "Preparing PBI for site_id 42\n",
            "Preparing PBI for site_id 49\n",
            "Preparing PBI for site_id 50\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>value</th>\n",
              "      <th>fcst</th>\n",
              "      <th>site_id</th>\n",
              "      <th>model</th>\n",
              "      <th>best_algorithm</th>\n",
              "      <th>train_start_date</th>\n",
              "      <th>train_end_date</th>\n",
              "      <th>test_start_date</th>\n",
              "      <th>test_end_date</th>\n",
              "      <th>energy_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>4.745603e+05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19</td>\n",
              "      <td>model_01_thermal</td>\n",
              "      <td>RF_Regressor</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>2017-03-06</td>\n",
              "      <td>2017-03-07</td>\n",
              "      <td>2022-12-31</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>6.370880e+05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42</td>\n",
              "      <td>model_01_thermal</td>\n",
              "      <td>RF_Regressor</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>2017-03-26</td>\n",
              "      <td>2017-03-27</td>\n",
              "      <td>2022-12-31</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>3.668250e+05</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22</td>\n",
              "      <td>model_01_thermal</td>\n",
              "      <td>LR_Regressor</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>2017-03-06</td>\n",
              "      <td>2017-03-07</td>\n",
              "      <td>2022-12-31</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-11-03</td>\n",
              "      <td>2.454051e+06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22</td>\n",
              "      <td>model_01_thermal</td>\n",
              "      <td>LR_Regressor</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>2017-03-06</td>\n",
              "      <td>2017-03-07</td>\n",
              "      <td>2022-12-31</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-11-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19</td>\n",
              "      <td>model_01_thermal</td>\n",
              "      <td>RF_Regressor</td>\n",
              "      <td>2015-11-02</td>\n",
              "      <td>2017-03-06</td>\n",
              "      <td>2017-03-07</td>\n",
              "      <td>2022-12-31</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   timestamp         value  fcst  site_id             model best_algorithm  \\\n",
              "0 2015-11-02  4.745603e+05   NaN       19  model_01_thermal   RF_Regressor   \n",
              "1 2015-11-02  6.370880e+05   NaN       42  model_01_thermal   RF_Regressor   \n",
              "2 2015-11-02  3.668250e+05   NaN       22  model_01_thermal   LR_Regressor   \n",
              "3 2015-11-03  2.454051e+06   NaN       22  model_01_thermal   LR_Regressor   \n",
              "4 2015-11-03  0.000000e+00   NaN       19  model_01_thermal   RF_Regressor   \n",
              "\n",
              "  train_start_date train_end_date test_start_date test_end_date  energy_diff  \n",
              "0       2015-11-02     2017-03-06      2017-03-07    2022-12-31          NaN  \n",
              "1       2015-11-02     2017-03-26      2017-03-27    2022-12-31          NaN  \n",
              "2       2015-11-02     2017-03-06      2017-03-07    2022-12-31          NaN  \n",
              "3       2015-11-02     2017-03-06      2017-03-07    2022-12-31          NaN  \n",
              "4       2015-11-02     2017-03-06      2017-03-07    2022-12-31          NaN  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pbi = pd.DataFrame({})\n",
        "for m in list(dict_results.keys()):\n",
        "    for s in list(dict_results[m].keys()):\n",
        "        print('Preparing PBI for', id, s)\n",
        "        df_historical_data = dict_results[m][s]['historical_data'].reset_index(drop=True)\n",
        "        df_forecast = dict_results[m][s]['forecast'].reset_index(drop=True)\n",
        "        df_merged = pd.merge(df_historical_data, df_forecast, on = date_var, how='outer', validate='1:1')\n",
        "        df_merged.loc[:, id] = s\n",
        "        df_merged.loc[:, 'model'] = m\n",
        "        df_merged.loc[:, 'best_algorithm'] = dict_results[m][s]['best_algorithm'] \n",
        "        df_merged.loc[:, 'train_start_date'] = dict_results[m][s]['train_start_date'] \n",
        "        df_merged.loc[:, 'train_end_date'] = dict_results[m][s]['train_end_date'] \n",
        "        df_merged.loc[:, 'test_start_date'] = dict_results[m][s]['test_start_date'] \n",
        "        df_merged.loc[:, 'test_end_date'] = dict_results[m][s]['test_end_date'] \n",
        "        df_pbi = pd.concat([df_pbi, df_merged], axis=0, ignore_index=True)\n",
        "        df_pbi.loc[:, 'energy_diff'] = df_pbi['fcst'] - df_pbi[y]\n",
        "        df_pbi.sort_values(by= date_var, inplace=True)\n",
        "    \n",
        "df_pbi.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1985040e",
      "metadata": {},
      "source": [
        "### Compute KPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "edad5b22",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE: 347644.38\n",
            "MAPE: 0.91\n"
          ]
        }
      ],
      "source": [
        "df_pbi = Kpi.compute_error(df_pbi, 'fcst', y)\n",
        "df_pbi = Kpi.compute_absolute_error(df_pbi, 'fcst', y)\n",
        "df_pbi = Kpi.compute_absolute_percentage_error(df_pbi, 'fcst', y)\n",
        "\n",
        "print(\"MAE:\",  round(Kpi.compute_mae(df_pbi, 'fcst', y), 2))\n",
        "print(\"MAPE:\", round(Kpi.compute_mape(df_pbi, 'fcst', y), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_nrSxU-r-CNz",
      "metadata": {
        "id": "_nrSxU-r-CNz"
      },
      "source": [
        "### Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "NeLJE3BeI52l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "NeLJE3BeI52l",
        "outputId": "55195d02-857a-4388-e135-6ca50a4ca0dc"
      },
      "outputs": [],
      "source": [
        "y = 'value'\n",
        "for s in list(df_pbi[id].unique()):\n",
        "    best_model = df_pbi.loc[df_pbi[id] == s, 'best_algorithm'].unique()[0]\n",
        "    chart_title = f\"Energy prediction - {id}: {s} - algorithm used: {best_model}\"\n",
        "    df_plot = df_pbi.loc[df_pbi[id]==s, ]\n",
        "    saving_name = str(id) + '_' + str(s) + '_energy_prediction'\n",
        "    plot = Plots.sliding_fcst_plot(df_plot, y, 'fcst', chart_title, kpi=True)\n",
        "    df_plot.to_csv(os.path.join(root, cfg_path.data_dir.output_path, saving_name + \".csv\"))\n",
        "    plot.write_html(os.path.join(root, cfg_path.data_dir.plot_path, saving_name + \".html\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AbKOiffyAql8",
        "6YxUycDC9p0h"
      ],
      "name": "Analysis (1).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2b8f5b14411d0017ed363cef4929504a7281087d06f1b18c01da6e951b937e80"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 ('forecasting_energy')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
